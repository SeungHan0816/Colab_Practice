{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p02P42Tdmhax"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNkfU4Z0XmRJYBlCIA5leSO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeungHan0816/Colab_Practice/blob/main/Colab_Practice07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 07-01 순환 신경망(Recurrent Neural Network, RNN)"
      ],
      "metadata": {
        "id": "p02P42Tdmhax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 은닉 상태 : 이전까지 입력된 정보들을 요약하여 다음 계산에 전달하기 위한 중간 기억 벡터\n",
        "\n",
        "# (timesteps, input_size) 크기의 2D 텐서를 입력을 받음.\n",
        "# 실제로는 (batch_size, timesteps, input_size)의 크기의 3D 텐서를 입력을 받음.\n",
        "\n",
        "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
        "input_size = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
        "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
        "\n",
        "# np.random은 랜덤 관련 기능 모아둔 \"모듈\"\n",
        "# np.random.random(...)은 그 모듈 안에 있는 random() 함수\n",
        "\n",
        "inputs = np.random.random((timesteps, input_size)) # 입력에 해당되는 2D 텐서\n",
        "\n",
        "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화\n",
        "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬.\n",
        "\n",
        "print(hidden_state_t) # 8의 크기를 가지는 은닉 상태. 현재는 초기 은닉 상태로 모든 차원이 0의 값을 가짐.\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "Wx = np.random.random((hidden_size, input_size))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
        "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
        "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias)\n",
        "\n",
        "print(np.shape(Wx))\n",
        "print(np.shape(Wh))\n",
        "print(np.shape(b))\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "total_hidden_states = []\n",
        "\n",
        "# 메모리 셀 동작\n",
        "# np.dot은 행렬 곱\n",
        "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.\n",
        "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)\n",
        "  total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
        "\n",
        "  print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
        "  hidden_state_t = output_t\n",
        "\n",
        "# 리스트에 쌓아둔 은닉 상태들을 하나의 배열로 합쳐서 차원 올리기\n",
        "# 새 축(axis=0)을 기준으로 쌓는다.\n",
        "total_hidden_states = np.stack(total_hidden_states, axis = 0)\n",
        "# 출력 시 값을 깔끔하게 해준다.\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "# (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력.\n",
        "print(total_hidden_states)\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n"
      ],
      "metadata": {
        "id": "g4sakoEKmjsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2728c6d8-4129-434b-f0a8-5174b44985ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "(8, 4)\n",
            "(8, 8)\n",
            "(8,)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "(1, 8)\n",
            "(2, 8)\n",
            "(3, 8)\n",
            "(4, 8)\n",
            "(5, 8)\n",
            "(6, 8)\n",
            "(7, 8)\n",
            "(8, 8)\n",
            "(9, 8)\n",
            "(10, 8)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[[0.91990077 0.96246543 0.86038489 0.78653153 0.92924253 0.98944964\n",
            "  0.80188456 0.94470339]\n",
            " [0.99980968 0.99997092 0.9998881  0.99741157 0.99999095 0.99999464\n",
            "  0.99984992 0.99983684]\n",
            " [0.99988954 0.99995005 0.99985266 0.99752138 0.99999339 0.99999248\n",
            "  0.99984885 0.99990442]\n",
            " [0.9999666  0.99998776 0.99997285 0.99913669 0.99999719 0.99999803\n",
            "  0.99996182 0.99992761]\n",
            " [0.99995897 0.99997492 0.99993764 0.99884728 0.99999602 0.9999975\n",
            "  0.99994506 0.99994944]\n",
            " [0.99991096 0.9999748  0.99989082 0.99825824 0.999994   0.9999957\n",
            "  0.99989857 0.99992675]\n",
            " [0.99983582 0.99992107 0.99974942 0.99668643 0.99998442 0.9999781\n",
            "  0.99976785 0.99972944]\n",
            " [0.99992506 0.99998577 0.99996113 0.99846828 0.99999786 0.99999756\n",
            "  0.99992059 0.9999391 ]\n",
            " [0.99993401 0.99997915 0.99988597 0.99865878 0.99999421 0.9999975\n",
            "  0.99992638 0.99995932]\n",
            " [0.99997167 0.99998127 0.99992801 0.99922452 0.99999406 0.99999825\n",
            "  0.99996463 0.99996047]]\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 은닉 상태는 내부 계산 결과고, 그걸 바깥으로 내보내면 출력이 되는 거니까,\n",
        "# 출력과 은닉 상태를 같다고 말하는 경우가 있다\n",
        "\n",
        "input_size = 5 # 입력의 크기\n",
        "hidden_size = 8 # 은닉 상태의 크기\n",
        "\n",
        "# 입력 텐서는 (배치 크기 × 시점의 수 × 매 시점마다 들어가는 입력)의 크기\n",
        "# (batch_size, time_steps, input_size)\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "# RNN의 셀 인자로 입력의 크기, 은닉 상태의 크기를 정의해주고,\n",
        "# batch_first=True를 통해서 입력 텐서의 첫번째 차원이 배치 크기임을 알려줌\n",
        "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "# _something\t덜 중요하거나, 부가적인 용도로 쓰일 변수\n",
        "# outputs에 집중하고, status는 부가적인 정보\n",
        "\n",
        "# outputs는 여러 개의 은닉 상태(h₁, h₂, ..., hₜ)고,\n",
        "# status (정확히는 h_n)는 마지막 은닉 상태를 자동으로 반환해주는 RNN의 출력값 중 하나\n",
        "\n",
        "outputs, _status = cell(inputs)\n",
        "\n",
        "print(outputs.shape) # 모든 time-step의 hidden_state\n",
        "\n",
        "# 마지막 시점의 은닉 상태만 딱 하나 추출, 1개 배치, 1개 시점(마지막), 8차원 은닉 상태\n",
        "print(_status.shape) # 최종 time-step의 hidden_state\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "# 깊은 순환 신경망(Deep Recurrent Neural Network)\n",
        "\n",
        "# (batch_size, time_steps, input_size)\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
        "outputs, _status = cell(inputs)\n",
        "\n",
        "print(outputs.shape) # 모든 time-step의 hidden_state\n",
        "\n",
        "# 마지막 층의 모든 시점의 은닉 상태\n",
        "print(_status.shape) # (층의 개수, 배치 크기, 은닉 상태의 크기)\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "# 양방향 순환 신경망(Bidirectional Recurrent Neural Network)\n",
        "\n",
        "# (batch_size, time_steps, input_size)\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)\n",
        "\n",
        "outputs, _status = cell(inputs)\n",
        "\n",
        "print(outputs.shape) # (배치 크기, 시퀀스 길이, 은닉 상태의 크기 x 2)\n",
        "# 양방향의 은닉 상태 값들이 연결(concatenate)되었기 때문\n",
        "\n",
        "print(_status.shape) # (층의 개수 x 2, 배치 크기, 은닉 상태의 크기)\n",
        "# 정방향 기준으로는 마지막 시점에 해당되며\n",
        "# 역방향 기준에서는 첫번째 시점에 해당되는 시점의 출력값을 층의 개수만큼 쌓아 올린 결과값\n",
        "\n",
        "# yt는 출력층을 사용할 때, 위의 코드는 은닉층만 표현, 위의 코드에서 yt는 고려하지 않았다.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2rK91UNgBSp",
        "outputId": "ac698e41-0181-4476-bb81-abb6cc5c9d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n",
            "torch.Size([1, 1, 8])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "torch.Size([1, 10, 8])\n",
            "torch.Size([2, 1, 8])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "torch.Size([1, 10, 16])\n",
            "torch.Size([4, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 07-03 문자 단위 RNN(Char RNN): 실습 2개"
      ],
      "metadata": {
        "id": "F-crWjYqoSFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "\n",
        "# set(...): 중복된 문자를 제거하고 고유한 문자 집합을 만듬\n",
        "# list(...): 집합(set)을 리스트 바꿈.\n",
        "# sorted(...): 알파벳 순으로 정렬 (['!', 'a', 'e', 'l', 'p'])\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))\n",
        "\n",
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기(5개)\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1\n",
        "\n",
        "# char_vocab는 리스트임.\n",
        "# enumerate는 각 문자에 인덱스 붙여줌.\n",
        "# (c, i) for i, c in enumerate(char_vocab) : enumerate 결과를 (문자, 인덱스) 순서로 바꿔서 하나씩 꺼내 표현\n",
        "# dict는 키(key)와 값(value) 쌍을 저장하는 자료형\n",
        "# dict(...)는 이런 쌍들을 받아서 딕셔너리를 만들어주는 함수\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)\n",
        "\n",
        "# index_to_char = {} 는 숫자(인덱스)를 문자로 되돌리기 위한 딕셔너리\n",
        "# .items()\t딕셔너리의 (key, value) 쌍들을 반복\n",
        "# index_to_char[숫자] = 문자로 저장했기에 문자 → 숫자에서 숫자 → 문자로 바뀜\n",
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)\n",
        "\n",
        "# char_to_index[c], c는 현재 문자, 'a'면 char_to_index['a']는 → 1 (문자를 숫자로 바꿔주는 역할)\n",
        "# input_str이 'apple', ['a', 'p', 'p', 'l', 'e']로 하나씩 문자 꺼냄\n",
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)\n",
        "\n",
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "\n",
        "# RNN은 항상 입력 = [배치 수, 시퀀스 길이, 입력 벡터 차원]으로 받기에\n",
        "# 한 겹 감싸서 2차원을 3차원으로 확장할 준비\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "# np.eye(...)[x]\t해당 숫자 인덱스를 원-핫 벡터로 변환\n",
        "\n",
        "# np.eye(n)은 n × n 단위 행렬(identity matrix)을 자동으로 만들어줌(주로 원-핫 벡터 만들 때 인덱싱해서 사용)\n",
        "# np.array(...)는 직접 만든 리스트나 값을 넘파이 배열로 변환(리스트나 다른 구조를 묶어서 배열로 변환)\n",
        "# np.eye(n)는 원-핫 벡터 만들기, 수학용이고 np.array([...])는 리스트 → 넘파이 배열로 묶는 목적\n",
        "\n",
        "# 바깥에 [ ]가 한 번 더 있어 3차원으로 만들어 줌\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "\n",
        "# np.array([...])는 그 리스트를 하나의 커다란 배열로 통합(안할 경우 매우 느려지는 경고 표시 뜸!)\n",
        "x_one_hot = np.array(x_one_hot)\n",
        "print(x_one_hot)\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  # 모델 구조 정의\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "\n",
        "    # “지금 정의 중인 Net 클래스와 그 인스턴스(self)가 상속받은\n",
        "    # 부모 클래스의 초기화 함수(__init__())를 실행해줘!”\n",
        "    super(Net, self).__init__() # Net, self\t현재 클래스(Net)와 그 인스턴스(self)\n",
        "    self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "    self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "  # 데이터 흐름 정의 (입력 → RNN → FC → 출력)\n",
        "  def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "\n",
        "    # x는 전체 시퀀스에 대한 은닉 상태\n",
        "    # _status는 마지막 시점의 은닉 상태 (state)\n",
        "    # 입력 → RNN → 은닉 상태 x → FC(출력층) → 최종 출력 x\n",
        "    # RNN이 구한 전체 은닉 상태 x를 FC층(출력층)에 넣어서 예측 결과로 변환된 x가 최종 출력\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "net = Net(input_size, hidden_size, output_size) # ← 여기서 net이 실제 self가 되는 인스턴스 이름\n",
        "\n",
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서\n",
        "\n",
        "# -1 : \"전체 데이터 수는 유지하고, 두 번째 차원은 5로 하고, 나머지는 알아서 계산해줘!\"\n",
        "\n",
        "# 총 원소 수 = 1 × 5 × 5 = 25를 .view(-1, 5)하면 두 번째 차원을 5로 지정된 상태에\n",
        "# 전체 원소 수는 그대로 25개여야 하며 -1로 자동 계산해달라 했으니\n",
        "# 첫 번째 차원은 자동으로 계산되어 5가 나옴(첫 번째 차원 = 25 / 5 = 5)\n",
        "\n",
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환\n",
        "\n",
        "# Y의 총 원소는 1 × 5 = 5이고 -1로 전체 원소 수는 유지하며 1차원으로 자동 계산하니 5가 나옴.\n",
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # 손실 함수를 정의\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate) # 옵티마이저(기울기) 정의\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    # # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "\n",
        "    # axis=2 : 클래스 차원 각 시점에서 가장 높은 클래스 인덱스 추출\n",
        "    # 각 행(=각 시점의 출력)에서 가장 높은 값의 인덱스(번호)를 추출\n",
        "    result = outputs.data.numpy().argmax(axis=2)\n",
        "\n",
        "    # np.squeeze()는 \"차원을 줄이는 함수\"\n",
        "    # np.squeeze(result): [1, 5] 형태의 배열에서 불필요한 배치 차원(1) 제거 → [5]\n",
        "\n",
        "    # [index_to_char[c] for c in ...]: 각 인덱스를 문자로 복원 (예: 2 → 'p', 4 → 'l' 등\n",
        "\n",
        "    # join()은 리스트 요소들을 하나의 문자열로 합치는 함수\n",
        "    # ''는 각 요소 사이에 아무것도 넣지 말라는 뜻\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)\n",
        "\n"
      ],
      "metadata": {
        "id": "VuhWCZL5oytg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128953be-699c-41be-b147-d7cc77d2d588"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n",
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n",
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n",
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[[[0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1. 0.]\n",
            "  [0. 0. 1. 0. 0.]]]\n",
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "torch.Size([1, 5, 5])\n",
            "torch.Size([5, 5])\n",
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 loss:  1.5977658033370972 prediction:  [[0 0 0 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  !!!!!\n",
            "1 loss:  1.3395131826400757 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "2 loss:  1.1239253282546997 prediction:  [[4 4 4 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppep\n",
            "3 loss:  0.8881467580795288 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "4 loss:  0.7200797200202942 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "5 loss:  0.5773823261260986 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.4331150949001312 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.3163619041442871 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.24340927600860596 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.18831796944141388 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.1390560269355774 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.1031959056854248 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.08080574125051498 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.06644569337368011 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.055705975741147995 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.046494074165821075 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.038311947137117386 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.031275033950805664 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.025506606325507164 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.02096218429505825 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.017471449449658394 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.01482042670249939 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.01280616968870163 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.011260299012064934 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.010053151287138462 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0090892119333148 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.008300124667584896 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.007638023234903812 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.007070033811032772 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0065736426040530205 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.0061339279636740685 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0057405149564146996 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.005386635661125183 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.005067178048193455 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0047784773632884026 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.004517727997153997 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.004282412119209766 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.004070155322551727 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0038791541010141373 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.003707223804667592 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.003552653593942523 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0034137065522372723 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.003288737265393138 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.00317607494071126 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0030745447147637606 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0029826373793184757 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0028993168380111456 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0028235213831067085 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0027542596217244864 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.002690680790692568 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0026321953628212214 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0025779749266803265 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0025276413653045893 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.002480627503246069 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0024364590644836426 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.002394971204921603 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.002355689648538828 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0023184488527476788 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.002283036010339856 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0022492846474051476 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0022169812582433224 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.002186079043895006 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0021564350463449955 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.002127978717908263 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.002100614830851555 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0020742486231029034 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.002048903377726674 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.002024437068030238 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.00200080219656229 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.001978093758225441 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0019560980144888163 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0019347671186551452 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.001914196996949613 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0018942917231470346 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.001875028247013688 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0018563112244009972 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0018380930414423347 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.0018204452935606241 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0018032491207122803 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0017865285044535995 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0017702117329463363 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0017543230205774307 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0017387669067829847 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0017235202249139547 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0017086773877963424 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0016940723871812224 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.001679800683632493 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0016657902160659432 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0016519937198609114 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0016384590417146683 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0016251622000709176 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.001612007967196405 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0015991150867193937 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0015864366432651877 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0015738768270239234 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.001561507349833846 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0015493757091462612 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0015373151982203126 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0015254926402121782 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0015137412119656801 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
        "print(char_dic) # 공백도 여기서는 하나의 원소\n",
        "\n",
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "\n",
        "  # 슬라이싱 때문에 \"- sequence_length\" 한 것(160 - 10 = 150, range(0, 150)이면 0~149개가 되며\n",
        "  # i = 149일 때 i + 149 = 159, i + 149 + 1 = 160으로 단어가 안 깨짐.)\n",
        "\n",
        "  # 입력과 출력이 한 글자 밀린 이유는 → \"이전 문자들을 보고 다음 문자를 예측\"하는 구조\n",
        "  x_str = sentence[i:i + sequence_length]\n",
        "  y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "  print(i, x_str, '->', y_str)\n",
        "\n",
        "  x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "  y_data.append([char_dic[c] for c in y_str])  # y str to index\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "print(x_data[0]) # if you wan에 해당됨.\n",
        "print(y_data[0]) # f you want에 해당됨.\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "x_one_hot = np.array(x_one_hot)\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))\n",
        "\n",
        "print(X[0])\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "print(Y[0])\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  # num_layers=layers는 몇 층(layer)의 RNN을 쌓을 것인지를 설정하는 하이퍼파라미터\n",
        "  def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "      super(Net, self).__init__()\n",
        "      self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "      self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x, _status = self.rnn(x)\n",
        "      x = self.fc(x)\n",
        "      return x\n",
        "\n",
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다.\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서\n",
        "\n",
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환.(dic_size = 25)\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "# 레이블 데이터는 (170, 10)의 크기를 가짐, 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정\n",
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "for i in range(175):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "  loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # results의 텐서 크기는 (170, 10)\n",
        "  results = outputs.argmax(dim=2)\n",
        "  predict_str = \"\" # \"\"는 빈 문자열(empty string) 의미\n",
        "\n",
        "  # enumerate는 리스트나 문자열을 반복할 때, 각 요소에 번호(index)를 자동으로 붙여주는 함수\n",
        "  for j, result in enumerate(results):\n",
        "    input_sentence = ''.join([char_set[i] for i in x_data[0]])\n",
        "    if j == 0: #  첫 번째 시점에는 예측된 전체 시퀀스 결과를 전부 가져오지만\n",
        "      # char_set[t]를 통해 다시 문자로 변환\n",
        "      # ''.join(...)는 문자들을 하나의 문자열로 합침\n",
        "      predict_str += ''.join([char_set[t] for t in result])\n",
        "    else: # 그 다음에는 마지막 글자만 하나씩 하나씩 이어붙이는 방식\n",
        "      # result[-1]은 그 리스트의 마지막 원소\n",
        "      # 시점 j=1 이후부터는 문장을 한 글자씩 예측하니\n",
        "      # 예측한 마지막 글자 하나만 꺼내서 최종 출력 문자열에 덧붙이는 역할\n",
        "      predict_str += char_set[result[-1]]\n",
        "\n",
        "  predict_str = input_sentence[0] + predict_str  # 첫 글자 'i'를 앞에 붙임\n",
        "  print(predict_str)\n",
        "\n",
        "\n",
        "# RNN은 “입력의 첫 글자를 보고 → 그 다음 글자를 예측\n",
        "# 그다음 글자까지 보고 → 그 다음 글자 또 예측, 이런 식으로 하나씩 다음 시점을 예측해가는 구조\n",
        "\n",
        "# 입력 시퀀스 \"if you wan\" → 출력 시퀀스 \"f you want\"\n",
        "# 'i'는 모델이 직접 예측한 적은 없음 (입력으로만 주어짐), 훈련도 \"f\"부터 예측하는 식으로 되어 있음\n",
        "# 훈련 중에 'i'를 출력하는 학습이 안 됨,\n",
        "# 그래서 테스트 시점에도 'i'를 출력하는 방법을 모르고, 임의의 글자 'g'를 뽑아냄\n",
        "\n",
        "# input_sentence = ''.join([char_set[i] for i in x_data[0]])을 함으로써 input_sentence를 정의함!\n",
        "# predict_str = input_sentence[0] + predict_str 이걸 함으로써 'i'를 첫 글자로 넣음(예측 가능해짐!)\n",
        "# 처음에는 이상한 예측을 하지만 마지막 에포크에서는 꽤 정확한 문자을 생성했음.(학습률 0.01, epoch 175)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJb8PGT9YaqV",
        "outputId": "6508a00a-a876-43d6-ab07-c58b38cdbe0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'u': 0, 'b': 1, 't': 2, 'a': 3, 'd': 4, 'c': 5, '.': 6, ',': 7, 'k': 8, 'y': 9, 'h': 10, 's': 11, 'r': 12, 'o': 13, ' ': 14, 'f': 15, 'i': 16, 'e': 17, 'g': 18, 'p': 19, 'l': 20, 'n': 21, \"'\": 22, 'm': 23, 'w': 24}\n",
            "문자 집합의 크기 : 25\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[16, 15, 14, 9, 13, 0, 14, 24, 3, 21]\n",
            "[15, 14, 9, 13, 0, 14, 24, 3, 21, 2]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0.]])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "tensor([15, 14,  9, 13,  0, 14, 24,  3, 21,  2])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "torch.Size([170, 10, 25])\n",
            "torch.Size([1700, 25])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "iggggggggg,ggggggggggdgglgggggggdggggggggggggggggglggggggggggggggggglgggggggggggggdggggggggdggglgggggggggggggggg,,ggg,gggggggggggg,gggggg,gggdg,ggggggglg,ggglgggggg,guggggggggglggg\n",
            "ideddddddgdgdlldggdeddlgdgledddddldddgdglldgdgddggddlddlglddllddeggldlgdldgdllgddddldddlddedglgdedlgddddllgddldgddgggldglglddlgddgldgdddledgedgdglddlgddddgdgdddgedldddedldgldgddddd\n",
            "idedd     d    d  d  d  d       d       d                d                     d  d        d   d d   d     d    d   d           d    d d  d  d        d           ddd          d    \n",
            "id                                                                                                                                                                                  \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                                                                                                                                                   \n",
            "i                                                          t                                                                t                      t                                \n",
            "i           t        t t     t                             t          t        t                 t         t   t t t t t    t     t    t       t t t                        t       \n",
            "i           t        t t     t t t      t                  t          t        t t t        t    t         t t t t t t t    t     t    t       t t t           t         t  t       \n",
            "i      t    t    t   t t     t t t      t         t        to         t    t   t t t        t    to    t   t t t t t t t    to    t    to      t t to          t         t  t       \n",
            "i      t    to   t   t t     t t t      t         to       to         t    t t t t t        to   to    t   to  t t t t t    to    th   to      t t th          t         t  th      \n",
            "i   o  to   th   t   t t     to  t    t t         to       th         to   tot to  t        to   th        to  t t t t to   th    th   th      tot th          t         t  th      \n",
            "i   o   o   the  t   toto    th  t    t th        toe   e  the       oto   to  to  to       toe  the       the t t toe toe  the   the  the     tht the   t e   t         t  the  o  \n",
            "i  toe  oe  the  t e totoe   the toa  t theto   t toe   e  the       othe  to  to  toa      toe  the    o  the t totha toe  the  ethe  the  e  tht the   t e   t         to the  o  \n",
            "i  toe  oe  the  t e totoe   the toa  t theto  ot the   e  the  e   tothe  ths the toa      toe  the  t ot the t tot a the  thet ethe  the oe  tht the  ot e   to     t  to the  os \n",
            "i  toe toe  the  t e totoe   the toa  t thetos  t the   e  the  e   tothe  tos tos toa      toe  the  t os the t tot a the  thet ethe  the  e  tht the  ot e   tot    t  to the  os \n",
            "i  toe toe  the  t t totoe   toe toa  t the os  t the t e  the      tothe  tot tos toa      toe  the  tont the t tot a the  the  ethe  the  e  tot the  ot e   tot    t  to the  os \n",
            "i  toe ton  the  t t totot   ton t a  t tht os  t toe t e  the    e tothe  tot tot than     toe  the  tont the t tot a the  the  ethe  the  e  tot the   t t t tot    t  to the  ot \n",
            "i  toe ton  thn  t t totot   ton t ao t tht os  t toe t er ton    e t thn  tot tot than     toe  ton  tant ton t tot t the  thn  etoe  toe  e  tot the   t ent tot    t  to the  ot \n",
            "i  ton ton  ton  t t totot   ton t ao t tht ns  e toe t er ton    e t ton  tot ton thaa     toem ton  tans ton t tot t them ton  etoe  ton oe  tot the  ot ent tot n  t n o toe  ot \n",
            "i  ton ton  ton  t t totot   ton t aa t tot os  e toe t er ton o  ent ton  tot ton than     toem ton  tans ton t aot t them ton  etoe  ton oe  tot the  nt en  tot n  t noo toe  ot \n",
            "i  ton ton  ton  t t aotot   ton t aa t tot os  e toe t er ton o  ent ton  aos ton t an     toem ton  tans ton t aot r them ton  etoem ton oe  tot toem nt en  tot n  t noo toem ot \n",
            "i  ton ton  ton  t t aotot   ton t aa t rot os  e toe t er ton o  ent ton  ans ton t an     toem ton  t ns ton t ant r them tont etoem ton oe  tot toem nt en  tot n  t  oo toem ot \n",
            "i  ton tonm tonp t t aotht   ton t aa t rot o   e the t er tonao  ent ton  ans ton t an     toem ton  t ns ton t ant r them tont etoem ton oe  tot toem nt en  t t ns t  on toem ot \n",
            "i  ton tonm tonp t   aotht   ton t ao t r t o t e the t er toeao  ent ton  ans ton t ans    toem ton  t ns ton t ant r them tont etoem tonpoe  tot them nt en  t t ns    an toem ot \n",
            "i  ton tonm tonh t   aotht   ton t ao t r t o t e the t er toeao  ent ton  ans ton t ans    toem tonh t ns ton t ant r them tonmh toem toehoe  tor them nt en  t t ns t  an toem ot \n",
            "i  ton tonm toeant   aotht   ton t ao t r t n t e the ther toebo ient ton  and ton t ans    toem tonhttand ton t ant r them tonmh toem toehoem tor them nt ers t t ns t  an toem ot \n",
            "i  ton tonm toeant   aotht   ton't ao t r thn t e the ther toebo ient ton  and ton t ansi   toem tonhttand ton t ant r them thnmh toem toehoem tor them nd ers t t ns t  an them ot \n",
            "i  ton tand toebnt r aotht   ton't ao t rnthnst e the ther toebo ient ton  and ton t ansi   toem tonh tand won t ant r them tonmh toem toeloem tor them nd ers t t ns t  an themaot \n",
            "i  too tand toebnt r andht r ton't ao t rn pnst e the ther toeco ient ton  and ton t ansil  toem tonh tand won t aut r ther tonth toem toeloem tor them nd ers t t ns t  an themaod \n",
            "i  too tand toebnt r andhi r ton't ao t r  pnst e the ther toeco ient ton  and won t ansil  toem tonh tand won t aut r ther tonsh toem toeloem tor them nd ers t   ns t  an them od \n",
            "i  too tand toebnt r andhi r ton't ao t r  tost e the ther toeco iert ton  and won't ansil  toem tosh tand won t aut r ther tonsh toem to loem tor them nd ers t   ns t  an themtod \n",
            "i  tooitand toebnt r andhi r ton't ao t r  tost e the ther toeco lert ton  and won't ansil  toem tosh tand won t aut r ther tonsh them to loem tor them nd ers t m ns t  an themtod \n",
            "i mtooitand toebnt r andhi r don't ao t rh tost e the ther toeco lert ton  and won't ansil  toem tosh tand won t aut r ther tonth toem to loem tor them nd ers t m ns t  an themtod \n",
            "i mtooitand to bnt r andhi , don't ao t rh tostle the ther toeco lert tond and won't ansil  toem tosh tand won t aut r ther tonth toem to loem tor them nd ers tem ns t  an themtod \n",
            "i mtooitant to lnt , anthi , don't aout rh tostle toe ther toeco lert tood and won't ansiln toem tosl tand wonmt aut rnther tonth toem to loec tor them nd ers tem nsit  an themtod \n",
            "i mtooitant to lnt , andhi , don't aout rh lostle toe ther toeco lent tood and won't ansiln toem tosl tand wonmt aut rnther tonsh toem to loec tor them nd ens tem nsit  on them od \n",
            "i mtooitant to lnt r andhi n don't aout rh lootle tor ther toeco lent tood and won't ansiln toem tosp  and wonmt aut rnther tonsh toem to loec tor them nd ens tem nsit  on them od \n",
            "i mtooitant to cutln andhi n don't aout rh lootle tor ther toeco lent tood and won't ansiln them tosks and wonmt aut rnther tonsh them to loec tor them nd ens tem nsit  on them od \n",
            "i mtooitant to cutln andhi n don't aout rh lootle tor ther toeco lent tood and won't ansiln them tosks and wonmt aut rnther tonsh them to loec tor them nd ens tmm nsit  on them od \n",
            "i mtooitant to cutln andhi n don't aout rh lootle tor ther to co lent tood and won't ansiln them tosks and donmt aut rnther tonch them to loec tor them nd ens tmm nsitm oo them od \n",
            "immtooitant to lutln andhi , don't aout rh lootle tor them to co lent tood and won't ansiln them tosks and donkt aut rnther tonch them to lonc tor them nd ens tmm nsity oo them od \n",
            "immtooitant to lutln andhi , don't aout rh leoile tor them to co lent tood and won't ansign them tosks and wonkt aut rnther to ch them to lon' tor them nd ens tmm nsity oo them od \n",
            "immtooitant to cutln andhi , don't aoum rh leoile tor them to co lent tood and won't ansign them tosks and wonkt aut rnther to ch them to lon' tor them nd ens tmm nsity oo them od \n",
            "immtooitant to cutld andhi , don't aoum rh leoile tor ther to co lent tood and won't ansign them tosks and wonkt aut rnther to ch them to lon' tor them nd ens tmm nsity oo them od \n",
            "immto itant to cuild andhil, don't aoum rh leople tor ther to co lent tood and won't ansign them tosks and wonkt aut rnther to ch them to long tor them nd ens tmm nsity oo them od \n",
            "immto itant to cuild andhil, don't aoum dh leople torether to co lent wood and won't ansign them tosks and wonkt aut rnther to ch them to long tor them nd ens tmm nsity oo them od \n",
            "immdo itant to cuild andhil, don't aoum dh people torether to co lent wood and won't ansign them tosks and wonkt aut rnther to ch them to long tor them nd ens tmm nsity oo the  od \n",
            "immdonitant to cuild andhip, don't aoum dh people torether to co lent wood and won't ansign them tosks and wonkt aut rnther to ch them to long tor them nd ens tmm nsity oo the  od \n",
            "immaonitant to build andhip, don't aoum rh people torether to co lent wood and won't ansign them tasks and wonk, aut rnther to ch them to long tor them nd ens tmm nsity oo the  od \n",
            "immaonitant to luild andhip, don't aoum rh people torether to co lent wood and won't ansign them tasks and wonk, aut rnther to ch them to long tor the  nd ens tmm nsity oo the  od \n",
            "immaonitant to luild andhip, don't aoum rh people torether to co lent wood and won't ansign them tasks and wonk, aut rnther to ch them to long tor the  nd ens tmm nsity oo the  od \n",
            "immao itant to luild andhip, don't aoum rh people torether to co lent wood and won't ansign them tasks and wonk, aut rnther toach them to long tor the  nd ens tmm nsity oo the  ed \n",
            "immao itant to luild a ship, don't aoum rp people torether to co lest wood and won't ansign them tasks and work, aut rather toach them to long tor the  nd ens tmmensity oo the sed \n",
            "immao  tant to luild a ship, don't aoum rp people torether to collect wood and won't ansign them tasks and work, dut rather toach them to long tor the end ens tmmensity oo the sed \n",
            "immao  tant to luild a ship, don't doum rp people torether to collect wood and won't ansign them tasks and work, but rather toach them ta long for the end ens tmmensity oo the sed \n",
            "immao  tant to luild a ship, don't doum rp people torether to collect wood and won't ansign them tasks and work, but rather toach them ta long for the endlens tmmensity oo the sed \n",
            "immao  tant to luild a ship, don't doum rp people torether to collect wood and won't ansign them tasks and work, but rather toach them ta long for the endlens tmmensity oo the sed \n",
            "igmao  tant to luild a ship, don't doum rp people torether to collect wood and don't assign them tasks and work, but rather toach them ta long for the endlens tmmensity oo the sed \n",
            "igmao  tant to luild a ship, don't doum rp people torether to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endlens immensity of the sed \n",
            "igmaou tant to luild a ship, don't doum rp people torether to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endlens immensity of the sed \n",
            "igmaou tant to luild anship, don't doum rp people torether to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endlens immensity of the sed \n",
            "igmaou tant to build a ship, don't doum rp people together to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endlens immensity of the sed \n",
            "igmaou tant to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endless immensity of the sed \n",
            "igmaou tant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sed \n",
            "igmaou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sed \n",
            "igmaou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sed \n",
            "igmaou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sed \n",
            "igmaou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endless immensity of the sed \n",
            "ig aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endless immensity of the sed \n",
            "ig aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sed \n",
            "ig aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sed \n",
            "ig aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the seds\n",
            "ig aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the seds\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seds\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seds\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seds\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seds\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seds\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seds\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seds\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sers\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sers\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sers\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "it aou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    }
  ]
}